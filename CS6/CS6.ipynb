{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91659aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086ac320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># label</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.346368</td>\n",
       "      <td>0.416306</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.427493</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>1.989833</td>\n",
       "      <td>0.344530</td>\n",
       "      <td>1.566297</td>\n",
       "      <td>...</td>\n",
       "      <td>4.105282</td>\n",
       "      <td>0.267826</td>\n",
       "      <td>0.378718</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>3.406367</td>\n",
       "      <td>4.350537</td>\n",
       "      <td>-0.352571</td>\n",
       "      <td>1.130032</td>\n",
       "      <td>2.227706</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.708236</td>\n",
       "      <td>-0.319394</td>\n",
       "      <td>-1.241873</td>\n",
       "      <td>-0.887231</td>\n",
       "      <td>-0.871906</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>-1.038225</td>\n",
       "      <td>0.655748</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.178141</td>\n",
       "      <td>-0.877361</td>\n",
       "      <td>-1.483769</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-1.693781</td>\n",
       "      <td>-0.545062</td>\n",
       "      <td>-0.299118</td>\n",
       "      <td>-0.662942</td>\n",
       "      <td>-0.193019</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.360693</td>\n",
       "      <td>1.794174</td>\n",
       "      <td>0.264738</td>\n",
       "      <td>-0.472273</td>\n",
       "      <td>-0.292344</td>\n",
       "      <td>-1.054221</td>\n",
       "      <td>-1.150495</td>\n",
       "      <td>1.423404</td>\n",
       "      <td>1.270098</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.199511</td>\n",
       "      <td>0.539020</td>\n",
       "      <td>-1.590629</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.543636</td>\n",
       "      <td>-0.937456</td>\n",
       "      <td>-0.300344</td>\n",
       "      <td>-0.523262</td>\n",
       "      <td>-1.506304</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.377914</td>\n",
       "      <td>-0.103932</td>\n",
       "      <td>-0.649434</td>\n",
       "      <td>-2.125015</td>\n",
       "      <td>-1.643797</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>1.011112</td>\n",
       "      <td>-1.040340</td>\n",
       "      <td>-0.541991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463763</td>\n",
       "      <td>-0.006583</td>\n",
       "      <td>1.089122</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.276348</td>\n",
       "      <td>-0.409272</td>\n",
       "      <td>-0.349926</td>\n",
       "      <td>-0.307123</td>\n",
       "      <td>0.529698</td>\n",
       "      <td>1250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067436</td>\n",
       "      <td>-0.636762</td>\n",
       "      <td>-0.620166</td>\n",
       "      <td>-0.062551</td>\n",
       "      <td>1.588715</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>-0.595304</td>\n",
       "      <td>-1.238987</td>\n",
       "      <td>0.336844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.552837</td>\n",
       "      <td>-1.418494</td>\n",
       "      <td>-0.562982</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>0.881802</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>1.560950</td>\n",
       "      <td>-0.150760</td>\n",
       "      <td>-1.023889</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # label        f0        f1        f2        f3        f4        f5  \\\n",
       "0      1.0 -0.346368  0.416306  0.999236  0.475342  0.427493 -0.005984   \n",
       "1      1.0  1.708236 -0.319394 -1.241873 -0.887231 -0.871906 -0.005984   \n",
       "2      0.0 -0.360693  1.794174  0.264738 -0.472273 -0.292344 -1.054221   \n",
       "3      1.0 -0.377914 -0.103932 -0.649434 -2.125015 -1.643797 -0.005984   \n",
       "4      0.0 -0.067436 -0.636762 -0.620166 -0.062551  1.588715 -0.005984   \n",
       "\n",
       "         f6        f7        f8  ...       f18       f19       f20       f21  \\\n",
       "0  1.989833  0.344530  1.566297  ...  4.105282  0.267826  0.378718  1.743123   \n",
       "1 -0.001047 -1.038225  0.655748  ... -1.178141 -0.877361 -1.483769 -0.573682   \n",
       "2 -1.150495  1.423404  1.270098  ... -1.199511  0.539020 -1.590629 -0.573682   \n",
       "3  1.011112 -1.040340 -0.541991  ...  0.463763 -0.006583  1.089122 -0.573682   \n",
       "4 -0.595304 -1.238987  0.336844  ... -0.552837 -1.418494 -0.562982  1.743123   \n",
       "\n",
       "        f22       f23       f24       f25       f26    mass  \n",
       "0  3.406367  4.350537 -0.352571  1.130032  2.227706  1000.0  \n",
       "1 -1.693781 -0.545062 -0.299118 -0.662942 -0.193019   750.0  \n",
       "2 -0.543636 -0.937456 -0.300344 -0.523262 -1.506304   750.0  \n",
       "3 -0.276348 -0.409272 -0.349926 -0.307123  0.529698  1250.0  \n",
       "4  0.881802  0.002516  1.560950 -0.150760 -1.023889   750.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/austi/Documents/QTW/Case Study 6/all_train.csv.gz', compression='gzip')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e73f8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># label</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.346368</td>\n",
       "      <td>0.416306</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.427493</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>1.989833</td>\n",
       "      <td>0.344530</td>\n",
       "      <td>1.566297</td>\n",
       "      <td>...</td>\n",
       "      <td>4.105282</td>\n",
       "      <td>0.267826</td>\n",
       "      <td>0.378718</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>3.406367</td>\n",
       "      <td>4.350537</td>\n",
       "      <td>-0.352571</td>\n",
       "      <td>1.130032</td>\n",
       "      <td>2.227706</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.708236</td>\n",
       "      <td>-0.319394</td>\n",
       "      <td>-1.241873</td>\n",
       "      <td>-0.887231</td>\n",
       "      <td>-0.871906</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>-1.038225</td>\n",
       "      <td>0.655748</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.178141</td>\n",
       "      <td>-0.877361</td>\n",
       "      <td>-1.483769</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-1.693781</td>\n",
       "      <td>-0.545062</td>\n",
       "      <td>-0.299118</td>\n",
       "      <td>-0.662942</td>\n",
       "      <td>-0.193019</td>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.360693</td>\n",
       "      <td>1.794174</td>\n",
       "      <td>0.264738</td>\n",
       "      <td>-0.472273</td>\n",
       "      <td>-0.292344</td>\n",
       "      <td>-1.054221</td>\n",
       "      <td>-1.150495</td>\n",
       "      <td>1.423404</td>\n",
       "      <td>1.270098</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.199511</td>\n",
       "      <td>0.539020</td>\n",
       "      <td>-1.590629</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.543636</td>\n",
       "      <td>-0.937456</td>\n",
       "      <td>-0.300344</td>\n",
       "      <td>-0.523262</td>\n",
       "      <td>-1.506304</td>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.377914</td>\n",
       "      <td>-0.103932</td>\n",
       "      <td>-0.649434</td>\n",
       "      <td>-2.125015</td>\n",
       "      <td>-1.643797</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>1.011112</td>\n",
       "      <td>-1.040340</td>\n",
       "      <td>-0.541991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463763</td>\n",
       "      <td>-0.006583</td>\n",
       "      <td>1.089122</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.276348</td>\n",
       "      <td>-0.409272</td>\n",
       "      <td>-0.349926</td>\n",
       "      <td>-0.307123</td>\n",
       "      <td>0.529698</td>\n",
       "      <td>1250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067436</td>\n",
       "      <td>-0.636762</td>\n",
       "      <td>-0.620166</td>\n",
       "      <td>-0.062551</td>\n",
       "      <td>1.588715</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>-0.595304</td>\n",
       "      <td>-1.238987</td>\n",
       "      <td>0.336844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.552837</td>\n",
       "      <td>-1.418494</td>\n",
       "      <td>-0.562982</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>0.881802</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>1.560950</td>\n",
       "      <td>-0.150760</td>\n",
       "      <td>-1.023889</td>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.617264</td>\n",
       "      <td>-0.537084</td>\n",
       "      <td>-1.275867</td>\n",
       "      <td>0.650799</td>\n",
       "      <td>-1.511621</td>\n",
       "      <td>0.850488</td>\n",
       "      <td>0.596391</td>\n",
       "      <td>-0.054678</td>\n",
       "      <td>0.728849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664328</td>\n",
       "      <td>-0.960709</td>\n",
       "      <td>-0.894011</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>-0.426198</td>\n",
       "      <td>-0.324286</td>\n",
       "      <td>-0.432739</td>\n",
       "      <td>1.340297</td>\n",
       "      <td>0.267774</td>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511357</td>\n",
       "      <td>0.270927</td>\n",
       "      <td>0.085989</td>\n",
       "      <td>-0.243802</td>\n",
       "      <td>-1.035668</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>-0.127219</td>\n",
       "      <td>0.721426</td>\n",
       "      <td>1.404479</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015462</td>\n",
       "      <td>1.367217</td>\n",
       "      <td>-1.053815</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-1.907798</td>\n",
       "      <td>0.194661</td>\n",
       "      <td>-0.190621</td>\n",
       "      <td>0.027776</td>\n",
       "      <td>-0.316018</td>\n",
       "      <td>1250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.062408</td>\n",
       "      <td>-0.987203</td>\n",
       "      <td>0.570667</td>\n",
       "      <td>1.517195</td>\n",
       "      <td>0.639548</td>\n",
       "      <td>-1.054221</td>\n",
       "      <td>1.115239</td>\n",
       "      <td>1.261928</td>\n",
       "      <td>-1.009308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790842</td>\n",
       "      <td>0.892545</td>\n",
       "      <td>-0.192816</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>0.973622</td>\n",
       "      <td>1.034964</td>\n",
       "      <td>-0.340661</td>\n",
       "      <td>-0.181193</td>\n",
       "      <td>1.877042</td>\n",
       "      <td>1500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.659131</td>\n",
       "      <td>1.096223</td>\n",
       "      <td>0.562821</td>\n",
       "      <td>1.627193</td>\n",
       "      <td>0.767236</td>\n",
       "      <td>-1.054221</td>\n",
       "      <td>1.079999</td>\n",
       "      <td>0.155488</td>\n",
       "      <td>-1.412207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.671423</td>\n",
       "      <td>-0.308908</td>\n",
       "      <td>-0.568336</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>1.043119</td>\n",
       "      <td>1.270350</td>\n",
       "      <td>0.217405</td>\n",
       "      <td>0.120213</td>\n",
       "      <td>1.073020</td>\n",
       "      <td>1500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.744152</td>\n",
       "      <td>-0.908839</td>\n",
       "      <td>-0.770454</td>\n",
       "      <td>1.008405</td>\n",
       "      <td>-1.054221</td>\n",
       "      <td>-0.370155</td>\n",
       "      <td>0.296837</td>\n",
       "      <td>-1.492524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352893</td>\n",
       "      <td>0.671047</td>\n",
       "      <td>0.176512</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>0.314739</td>\n",
       "      <td>0.304983</td>\n",
       "      <td>0.425471</td>\n",
       "      <td>-0.612085</td>\n",
       "      <td>-0.925097</td>\n",
       "      <td>499.999969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000000 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         # label        f0        f1        f2        f3        f4        f5  \\\n",
       "0            1.0 -0.346368  0.416306  0.999236  0.475342  0.427493 -0.005984   \n",
       "1            1.0  1.708236 -0.319394 -1.241873 -0.887231 -0.871906 -0.005984   \n",
       "2            0.0 -0.360693  1.794174  0.264738 -0.472273 -0.292344 -1.054221   \n",
       "3            1.0 -0.377914 -0.103932 -0.649434 -2.125015 -1.643797 -0.005984   \n",
       "4            0.0 -0.067436 -0.636762 -0.620166 -0.062551  1.588715 -0.005984   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "6999995      0.0  1.617264 -0.537084 -1.275867  0.650799 -1.511621  0.850488   \n",
       "6999996      0.0 -0.511357  0.270927  0.085989 -0.243802 -1.035668 -0.005984   \n",
       "6999997      1.0  0.062408 -0.987203  0.570667  1.517195  0.639548 -1.054221   \n",
       "6999998      1.0  1.659131  1.096223  0.562821  1.627193  0.767236 -1.054221   \n",
       "6999999      1.0  0.002034  0.744152 -0.908839 -0.770454  1.008405 -1.054221   \n",
       "\n",
       "               f6        f7        f8  ...       f18       f19       f20  \\\n",
       "0        1.989833  0.344530  1.566297  ...  4.105282  0.267826  0.378718   \n",
       "1       -0.001047 -1.038225  0.655748  ... -1.178141 -0.877361 -1.483769   \n",
       "2       -1.150495  1.423404  1.270098  ... -1.199511  0.539020 -1.590629   \n",
       "3        1.011112 -1.040340 -0.541991  ...  0.463763 -0.006583  1.089122   \n",
       "4       -0.595304 -1.238987  0.336844  ... -0.552837 -1.418494 -0.562982   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "6999995  0.596391 -0.054678  0.728849  ...  0.664328 -0.960709 -0.894011   \n",
       "6999996 -0.127219  0.721426  1.404479  ... -1.015462  1.367217 -1.053815   \n",
       "6999997  1.115239  1.261928 -1.009308  ...  0.790842  0.892545 -0.192816   \n",
       "6999998  1.079999  0.155488 -1.412207  ... -0.671423 -0.308908 -0.568336   \n",
       "6999999 -0.370155  0.296837 -1.492524  ...  0.352893  0.671047  0.176512   \n",
       "\n",
       "              f21       f22       f23       f24       f25       f26  \\\n",
       "0        1.743123  3.406367  4.350537 -0.352571  1.130032  2.227706   \n",
       "1       -0.573682 -1.693781 -0.545062 -0.299118 -0.662942 -0.193019   \n",
       "2       -0.573682 -0.543636 -0.937456 -0.300344 -0.523262 -1.506304   \n",
       "3       -0.573682 -0.276348 -0.409272 -0.349926 -0.307123  0.529698   \n",
       "4        1.743123  0.881802  0.002516  1.560950 -0.150760 -1.023889   \n",
       "...           ...       ...       ...       ...       ...       ...   \n",
       "6999995  1.743123 -0.426198 -0.324286 -0.432739  1.340297  0.267774   \n",
       "6999996 -0.573682 -1.907798  0.194661 -0.190621  0.027776 -0.316018   \n",
       "6999997 -0.573682  0.973622  1.034964 -0.340661 -0.181193  1.877042   \n",
       "6999998 -0.573682  1.043119  1.270350  0.217405  0.120213  1.073020   \n",
       "6999999 -0.573682  0.314739  0.304983  0.425471 -0.612085 -0.925097   \n",
       "\n",
       "                mass  \n",
       "0        1000.000000  \n",
       "1         750.000000  \n",
       "2         750.000000  \n",
       "3        1250.000000  \n",
       "4         750.000000  \n",
       "...              ...  \n",
       "6999995   750.000000  \n",
       "6999996  1250.000000  \n",
       "6999997  1500.000000  \n",
       "6999998  1500.000000  \n",
       "6999999   499.999969  \n",
       "\n",
       "[7000000 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9387117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# label\n",
       "1.0    3500879\n",
       "0.0    3499121\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['# label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9beba665",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column_name = '# label'  \n",
    "\n",
    "X = df.drop(columns=[label_column_name])\n",
    "y = df[label_column_name].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1db6ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# label    False\n",
       "f0         False\n",
       "f1         False\n",
       "f2         False\n",
       "f3         False\n",
       "f4         False\n",
       "f5         False\n",
       "f6         False\n",
       "f7         False\n",
       "f8         False\n",
       "f9         False\n",
       "f10        False\n",
       "f11        False\n",
       "f12        False\n",
       "f13        False\n",
       "f14        False\n",
       "f15        False\n",
       "f16        False\n",
       "f17        False\n",
       "f18        False\n",
       "f19        False\n",
       "f20        False\n",
       "f21        False\n",
       "f22        False\n",
       "f23        False\n",
       "f24        False\n",
       "f25        False\n",
       "f26        False\n",
       "mass       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1908d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b7facbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60a1ffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "87500/87500 [==============================] - 244s 3ms/step - loss: 0.3009 - accuracy: 0.8621 - val_loss: 0.2786 - val_accuracy: 0.8734\n",
      "Epoch 2/20\n",
      "87500/87500 [==============================] - 241s 3ms/step - loss: 0.2923 - accuracy: 0.8676 - val_loss: 0.2768 - val_accuracy: 0.8748\n",
      "Epoch 3/20\n",
      "87500/87500 [==============================] - 241s 3ms/step - loss: 0.2901 - accuracy: 0.8689 - val_loss: 0.2737 - val_accuracy: 0.8764\n",
      "Epoch 4/20\n",
      "87500/87500 [==============================] - 242s 3ms/step - loss: 0.2893 - accuracy: 0.8694 - val_loss: 0.2735 - val_accuracy: 0.8763\n",
      "Epoch 5/20\n",
      "87500/87500 [==============================] - 240s 3ms/step - loss: 0.2884 - accuracy: 0.8699 - val_loss: 0.2738 - val_accuracy: 0.8768\n",
      "Epoch 6/20\n",
      "87500/87500 [==============================] - 242s 3ms/step - loss: 0.2881 - accuracy: 0.8702 - val_loss: 0.2728 - val_accuracy: 0.8768\n",
      "Epoch 7/20\n",
      "87500/87500 [==============================] - 242s 3ms/step - loss: 0.2875 - accuracy: 0.8706 - val_loss: 0.2727 - val_accuracy: 0.8771\n",
      "Epoch 8/20\n",
      "87500/87500 [==============================] - 241s 3ms/step - loss: 0.2873 - accuracy: 0.8705 - val_loss: 0.2709 - val_accuracy: 0.8778\n",
      "Epoch 9/20\n",
      "87500/87500 [==============================] - 234s 3ms/step - loss: 0.2870 - accuracy: 0.8708 - val_loss: 0.2717 - val_accuracy: 0.8774\n",
      "Epoch 10/20\n",
      "87500/87500 [==============================] - 205s 2ms/step - loss: 0.2869 - accuracy: 0.8710 - val_loss: 0.2715 - val_accuracy: 0.8775\n",
      "Epoch 11/20\n",
      "87500/87500 [==============================] - 200s 2ms/step - loss: 0.2867 - accuracy: 0.8710 - val_loss: 0.2718 - val_accuracy: 0.8775\n",
      "Epoch 12/20\n",
      "87500/87500 [==============================] - 207s 2ms/step - loss: 0.2866 - accuracy: 0.8712 - val_loss: 0.2714 - val_accuracy: 0.8780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146037182e0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 4)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid') # sigmoidd is good for binary clasification\n",
    "])\n",
    "\n",
    "\n",
    "### want to test the below next\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape = (X_train.shape[1],)),\n",
    "#     tf.keras.layers.Dense(128, activation = 'relu'), \n",
    "#     tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "#     tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Dense(1, activation = 'sigmoid') # sigmoidd is good for binary clasification\n",
    "# ])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train,\n",
    "          epochs = 20,\n",
    "          batch_size = 64,\n",
    "          callbacks = [callback], \n",
    "          validation_data = (X_validation, y_validation))\n",
    "\n",
    "# accuracy maintains around 87% in the first epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e4a9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21875/21875 [==============================] - 31s 1ms/step - loss: 0.2706 - accuracy: 0.8783\n",
      "Test Accuracy: 87.83%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7302879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6895dc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_66 (Dense)            (None, 128)               3712      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,545\n",
      "Trainable params: 12,289\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d494a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21875/21875 [==============================] - 26s 1ms/step\n",
      "Confusion Matrix:\n",
      "[[295867  53982]\n",
      " [ 31194 318957]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a5aa48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(conf_matrix, \n",
    "            annot = True,\n",
    "            fmt = 'd',\n",
    "            cmap = 'Blues',\n",
    "            cbar = False,\n",
    "            xticklabels = ['0', '1'],\n",
    "            yticklabels = ['0', '1'])\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f57f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Should we train a random forest like capstone then run again on best features??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
